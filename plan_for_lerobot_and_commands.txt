I'm at a LeRobot hackathon, and getting two so101 arms first to:
1. teleoperate (done)
2. teleoperate with camera (done with loose webcam)
3. Record sample dataset (done)
4. Train a policy sample dataset
4.1 will cpu work for small dataset? if not, how to do it in cloud?
5. Test the policy on the robot
6. Work out FPS on CPU or inference over cloud
7. record big dataset of sock or whatever
8. train a policy on the big dataset
9. test it...




Below is a bunch of commands I'm saving to run again for my specific setup

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 4

python -m lerobot.setup_motors \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_leader"]'

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_follower"]'



python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm


python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/tty.usbmodem58760431551 \ 
    --robot.id=my_awesome_follower_arm

python -m lerobot.calibrate \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm


# what worked, 1 camera
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

# 2 cameras
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true


# record with 1 camera white socks
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_white_sock \
    --dataset.num_episodes=10 \
    --dataset.single_task="Grab the white sock"

# record with 2 camera black and white socks to container
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
    --dataset.num_episodes=60 \
    --dataset.single_task="Grab the black and white sock and put them into the red box" \
    --resume=true


python -m lerobot.replay \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --dataset.repo_id=${HF_USER}/record-test \
    --dataset.episode=0


# training that works in lightning ai
python lerobot/scripts/train.py \
  --dataset.repo_id=${HF_USER}/record-test4_with_permission_changed \
  --policy.type=act \
  --output_dir=outputs/train/act_record-test \
  --job_name=act_record-test3 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav
  
# white sock act
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_white_sock1 \
  --job_name=pick_up_white_sock1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# to resume add e.g.
--resume=true  --config_path=outputs/train/pick_up_blac
k_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model/train_config.json

# black and white socks into red box 2 cameras
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_black_and_white_sock_into_red_box \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_black_and_white_sock_into_red_box \
  --job_name=pick_up_black_and_white_sock_into_red_box1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# white sock smolvla
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.path=lerobot/smolvla_base \
  --batch_size=64 \
  --output_dir=outputs/train/pick_up_white_sock_smolvla_finetune \
  --job_name=pick_up_white_sock_smolvla_finetune \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav



# trying to eval on cpu but with new eval script. never worked.
python -m lerobot.scripts.eval \
        --robot.type=so101_follower \
        --robot.port=/dev/ttyACM0 \
        --robot.id=my_awesome_follower_arm \
        --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
        --policy_path=${HF_USER}/act_record-test \
        --device=cpu

WITH t4 updt_s:0.263 = 0.263 x 200 = 52.6 per 200 steps
With ag10 18.0.092 x 200 = 18.4s per 200 steps

smol vla updt_s:0.793 x 200 = 158.6 per 200 steps

# trying to do the same but with record eval way from docs. 
python -m lerobot.record  \
  --robot.type=so100_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.cameras="{ up: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}, side: {type: intelrealsense, serial_number_or_name: 233522074606, width: 640, height: 480, fps: 30}}" \
  --robot.id=my_awesome_follower_arm \
  --display_data=false \
  --dataset.repo_id=$HF_USER/eval_so100 \
  --dataset.single_task="Put lego brick into the transparent box" \
  --policy.path=${HF_USER}/act_record-test

# worked. 1 camera. wait did it?
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --control.policy.path=${HF_USER}/act_record-test \
  --dataset.repo_id=${HF_USER}/eval_act_record_test \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box, 100 episodes
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# between episodes to reload model
/home/ben/.cache/huggingface/lerobot/bearlover365/eval_pick_up_black_and_white_sock_into_red_box

huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential

HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

huggingface-cli upload \         
      ${HF_USER}/act_record-test \
      outputs/train/act_record-test/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload \         
      ${HF_USER}/pick_up_white_sock \
      outputs/train/pick_up_white_sock1/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_white_sock outputs/train/pick_up_white_sock1/checkpoints/last --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_white_sock_smolvla_finetune outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box outputs/train/pick_up_black_and_white_sock_into_red_box/checkpoints/last/pretrained_model --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 outputs/train/pick_up_black_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model --repo-type=model




# think it needs pretrained
huggingface-cli upload \
      ${HF_USER}/pick_up_white_sock_smolvla_finetune \
      outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last/pretrained_model \
      --repo-type=model


# eval that worked!!
python -m lerobot.record --config_path eval_config.yaml


(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 56,
        "range_min": 730,
        "range_max": 3434
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 793,
        "range_min": 867,
        "range_max": 3210
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -729,
        "range_min": 870,
        "range_max": 3076
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -50,
        "range_min": 945,
        "range_max": 3217
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 38,
        "range_min": 150,
        "range_max": 3750
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -530,
        "range_min": 2047,
        "range_max": 3482
    }




(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/teleoperators/so101_leader/my_awesome_leader_arm.json 
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 1180,
        "range_min": 794,
        "range_max": 3143
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 1368,
        "range_min": 834,
        "range_max": 3168
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -294,
        "range_min": 908,
        "range_max": 3130
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -676,
        "range_min": 857,
        "range_max": 3136
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 155,
        "range_min": 199,
        "range_max": 4035
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -2013,
        "range_min": 1590,
        "range_max": 2787
    }




latest, trying to fix homing offset problem of shoulder motor?


/home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 58,
        "range_min": 732,
        "range_max": 3430
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 813,
        "range_min": 834,
        "range_max": 3183
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -769,
        "range_min": 908,
        "range_max": 3121
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": 35,
        "range_min": 858,
        "range_max": 3128
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 33,
        "range_min": 162,
        "range_max": 3772
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -523,
        "range_min": 2041,
        "range_max": 3482
    }









########################################################
June 20th 2025, a week after hackathon, a pile of socks in ben's room, controlled lighting, all into red box.
########################################################

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pile_of_socks_into_red_hamper \
    --dataset.num_episodes=5 \
    --dataset.single_task="Grab all socks and put into red hamper"


python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pile_of_socks_into_red_hamper \
  --policy.type=act \
  --output_dir=outputs/train/pile_of_socks_into_red_hamper1 \
  --job_name=pile_of_socks_into_red_hamper1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav


huggingface-cli upload ${HF_USER}/pile_of_socks_into_red_hamper outputs/train/pile_of_socks_into_red_hamper1/checkpoints/last/pretrained_model --repo-type=model

# evaluate
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pile_of_socks_into_red_hamper \
  --dataset.repo_id=${HF_USER}/eval_pile_of_socks_into_red_hamper \
  --dataset.single_task="Grab all socks and put into red hamper" \
  --dataset.num_episodes=1 \
  --display_data=true


# TODO merge my two forks or start committing or something?
# TODO clear space on lightning and wandb, 
# TODO maybe don't save models to wandb?
# TODO understand when voice says episode over, it stops recording then, so can I reset sock position with arm or? 
# TODO ACT and smolVLA FPS. That matters right?
# TODO evaluate smolVLA, diffusion. 
# TODO train diffusion and compare to others
# TODO do train eval splits
# TODO how to save bashrc on lightning? probably just bashrc somewhere. lightning using fork
# TODO create jupyter notebooks instead of bash scripts
# TODO ville act feature detector
# TODO understand shreyas code
# TODO bigger batch size...?
# TODO IK
# TODO MCP
# omg i should focus on this, 100% accuracy in everything... and if not 100%, then just feed more data and train and intervene. 
# TODO HL-SERL
https://chatgpt.com/share/6856946e-157c-8002-87c0-396f8bce16a9
# TODO hl-serl simulation https://huggingface.co/docs/lerobot/hilserl_sim. nvidia
# TODO control arm in simulation with leader, get so-100 in sim
# train and evaluate in sim, train in both? how did russ tedrake do it?

# TODO profile policy. at end of README:
from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f"tmp/trace_schedule_{prof.step_num}.json")

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
    ),
    on_trace_ready=trace_handler
) as prof:
    with record_function("eval_policy"):
        for i in range(num_episodes):
            prof.step()
            # insert code to profile, potentially whole body of eval_policy function




# TODO run and fix and study all test pytests

=============================== short test summary info ===============================
FAILED tests/cameras/test_opencv.py::test_read[128x128] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_read[160x120] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_read[320x180] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_read[480x270] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_async_read[128x128] - TimeoutError: Timed out waiting for frame from camera OpenCVCamera(/home/ben/all_p...
FAILED tests/cameras/test_opencv.py::test_async_read[160x120] - TimeoutError: Timed out waiting for frame from camera OpenCVCamera(/home/ben/all_p...
FAILED tests/cameras/test_opencv.py::test_async_read[320x180] - TimeoutError: Timed out waiting for frame from camera OpenCVCamera(/home/ben/all_p...
FAILED tests/cameras/test_opencv.py::test_async_read[480x270] - TimeoutError: Timed out waiting for frame from camera OpenCVCamera(/home/ben/all_p...
FAILED tests/cameras/test_opencv.py::test_rotation[no_rot-128x128] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[no_rot-160x120] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[no_rot-320x180] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[no_rot-480x270] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot90-128x128] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot90-160x120] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot90-320x180] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot90-480x270] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot180-128x128] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot180-160x120] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot180-320x180] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot180-480x270] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot270-128x128] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot270-160x120] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot270-320x180] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/cameras/test_opencv.py::test_rotation[rot270-480x270] - RuntimeError: OpenCVCamera(/home/ben/all_projects/lerobot/tests/artifacts/cameras/...
FAILED tests/datasets/test_datasets.py::test_backward_compatibility[lerobot/pusht] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/datasets/test_datasets.py::test_backward_compatibility[lerobot/aloha_sim_insertion_human] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/datasets/test_datasets.py::test_backward_compatibility[lerobot/xarm_lift_medium] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/policies/test_policies.py::test_backward_compatibility[lerobot/xarm_lift_medium-tdmpc-policy_kwargs0-use_policy] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/policies/test_policies.py::test_backward_compatibility[lerobot/pusht-diffusion-policy_kwargs1-] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/policies/test_policies.py::test_backward_compatibility[lerobot/aloha_sim_insertion_human-act-policy_kwargs2-] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/policies/test_policies.py::test_backward_compatibility[lerobot/aloha_sim_insertion_human-act-policy_kwargs3-1000_steps] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
FAILED tests/rl/test_actor.py::test_establish_learner_connection_success - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor.py::test_establish_learner_connection_failure - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor.py::test_push_transitions_to_transport_queue - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor.py::test_transitions_stream - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor.py::test_interactions_stream - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor_learner.py::test_end_to_end_transitions_flow - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor_learner.py::test_end_to_end_interactions_flow - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor_learner.py::test_end_to_end_parameters_flow[small] - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_actor_learner.py::test_end_to_end_parameters_flow[large] - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_send_interactions - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_send_transitions - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_send_transitions_empty_stream - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_stream_parameters - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_stream_parameters_with_shutdown - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
FAILED tests/rl/test_learner_service.py::test_stream_parameters_waits_and_retries_on_empty_queue - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
ERROR examples/test_feetech_motor.py::test_motor_movement
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_single_transforms[ColorJitter-brightness-min_max_values0] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_single_transforms[ColorJitter-contrast-min_max_values1] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_single_transforms[ColorJitter-saturation-min_max_values2] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_single_transforms[ColorJitter-hue-min_max_values3] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_single_transforms[SharpnessJitter-sharpness-min_max_values4] - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/datasets/test_image_transforms.py::test_backward_compatibility_default_config - safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
ERROR tests/rl/test_learner_service.py::test_ready_method - RuntimeError: The grpc package installed is at version 1.70.0, but the generated c...
==== 46 failed, 564 passed, 17 skipped, 25 warnings, 8 errors

########################################################
# Visualizing datasets
########################################################

# TODO didn't work. 
# `visualize_dataset.py` uses rerun for a detailed, interactive 3D visualization.
# You can inspect camera streams, actions, and states frame-by-frame.
# This command visualizes episode 0 of your sock-hamper dataset.
# Use --video-backend=pyav to avoid torchcodec errors.
python lerobot/scripts/visualize_dataset.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episode-index=0 \
  --video-backend=pyav

# `visualize_dataset_html.py` starts a web server for a simpler HTML-based overview.
# After running, open http://localhost:9090 in your browser.
# This command visualizes your sock-hamper dataset.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper

# You can also select specific episodes to show in the HTML viewer.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episodes 0 2 4
