I'm at a LeRobot hackathon, and getting two so101 arms first to:
1. teleoperate (done)
2. teleoperate with camera (done with loose webcam)
3. Record sample dataset (done)
4. Train a policy sample dataset
4.1 will cpu work for small dataset? if not, how to do it in cloud?
5. Test the policy on the robot
6. Work out FPS on CPU or inference over cloud
7. record big dataset of sock or whatever
8. train a policy on the big dataset
9. test it...




Below is a bunch of commands I'm saving to run again for my specific setup

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 4

python -m lerobot.setup_motors \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_leader"]'

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_follower"]'



python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm


python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/tty.usbmodem58760431551 \ 
    --robot.id=my_awesome_follower_arm

python -m lerobot.calibrate \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm


# what worked, 1 camera
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

# 2 cameras
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true


# record with 1 camera white socks
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_white_sock \
    --dataset.num_episodes=10 \
    --dataset.single_task="Grab the white sock"

# record with 2 camera black and white socks to container
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
    --dataset.num_episodes=60 \
    --dataset.single_task="Grab the black and white sock and put them into the red box" \
    --resume=true


python -m lerobot.replay \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --dataset.repo_id=${HF_USER}/record-test \
    --dataset.episode=0


# training that works in lightning ai
python lerobot/scripts/train.py \
  --dataset.repo_id=${HF_USER}/record-test4_with_permission_changed \
  --policy.type=act \
  --output_dir=outputs/train/act_record-test \
  --job_name=act_record-test3 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav
  
# white sock act
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_white_sock1 \
  --job_name=pick_up_white_sock1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# to resume add e.g.
--resume=true  --config_path=outputs/train/pick_up_blac
k_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model/train_config.json

# black and white socks into red box 2 cameras
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_black_and_white_sock_into_red_box \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_black_and_white_sock_into_red_box \
  --job_name=pick_up_black_and_white_sock_into_red_box1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# white sock smolvla
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.path=lerobot/smolvla_base \
  --batch_size=64 \
  --output_dir=outputs/train/pick_up_white_sock_smolvla_finetune \
  --job_name=pick_up_white_sock_smolvla_finetune \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav



# trying to eval on cpu but with new eval script. never worked.
python -m lerobot.scripts.eval \
        --robot.type=so101_follower \
        --robot.port=/dev/ttyACM0 \
        --robot.id=my_awesome_follower_arm \
        --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
        --policy_path=${HF_USER}/act_record-test \
        --device=cpu

WITH t4 updt_s:0.263 = 0.263 x 200 = 52.6 per 200 steps
With ag10 18.0.092 x 200 = 18.4s per 200 steps
20000 x 0.092 = 1840 seconds = 0.51 hours = 30 minutes
60000 x 0.092 = 5520 seconds = 1.53 hours = 1 hour 32 minutes

smol vla updt_s:0.793 x 200 = 158.6 per 200 steps

# trying to do the same but with record eval way from docs. 
python -m lerobot.record  \
  --robot.type=so100_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.cameras="{ up: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}, side: {type: intelrealsense, serial_number_or_name: 233522074606, width: 640, height: 480, fps: 30}}" \
  --robot.id=my_awesome_follower_arm \
  --display_data=false \
  --dataset.repo_id=$HF_USER/eval_so100 \
  --dataset.single_task="Put lego brick into the transparent box" \
  --policy.path=${HF_USER}/act_record-test

# worked. 1 camera. wait did it?
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --control.policy.path=${HF_USER}/act_record-test \
  --dataset.repo_id=${HF_USER}/eval_act_record_test \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box, 100 episodes
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# between episodes to reload model
/home/ben/.cache/huggingface/lerobot/bearlover365/eval_pick_up_black_and_white_sock_into_red_box


HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

huggingface-cli upload \         
      ${HF_USER}/act_record-test \
      outputs/train/act_record-test/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload \         
      ${HF_USER}/pick_up_white_sock \
      outputs/train/pick_up_white_sock1/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_white_sock outputs/train/pick_up_white_sock1/checkpoints/last --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_white_sock_smolvla_finetune outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box outputs/train/pick_up_black_and_white_sock_into_red_box/checkpoints/last/pretrained_model --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 outputs/train/pick_up_black_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model --repo-type=model




# think it needs pretrained
huggingface-cli upload \
      ${HF_USER}/pick_up_white_sock_smolvla_finetune \
      outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last/pretrained_model \
      --repo-type=model


# eval that worked!!
python -m lerobot.record --config_path eval_config.yaml


(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 56,
        "range_min": 730,
        "range_max": 3434
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 793,
        "range_min": 867,
        "range_max": 3210
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -729,
        "range_min": 870,
        "range_max": 3076
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -50,
        "range_min": 945,
        "range_max": 3217
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 38,
        "range_min": 150,
        "range_max": 3750
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -530,
        "range_min": 2047,
        "range_max": 3482
    }




(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/teleoperators/so101_leader/my_awesome_leader_arm.json 
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 1180,
        "range_min": 794,
        "range_max": 3143
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 1368,
        "range_min": 834,
        "range_max": 3168
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -294,
        "range_min": 908,
        "range_max": 3130
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -676,
        "range_min": 857,
        "range_max": 3136
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 155,
        "range_min": 199,
        "range_max": 4035
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -2013,
        "range_min": 1590,
        "range_max": 2787
    }




latest, trying to fix homing offset problem of shoulder motor?


/home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 58,
        "range_min": 732,
        "range_max": 3430
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 813,
        "range_min": 834,
        "range_max": 3183
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -769,
        "range_min": 908,
        "range_max": 3121
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": 35,
        "range_min": 858,
        "range_max": 3128
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 33,
        "range_min": 162,
        "range_max": 3772
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -523,
        "range_min": 2041,
        "range_max": 3482
    }









########################################################
June 20th 2025, a week after hackathon, a pile of socks in ben's room, controlled lighting, all into red box.
########################################################

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

# run in both laptop and in cloud
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pile_of_socks_into_red_hamper \
    --dataset.num_episodes=5 \
    --dataset.single_task="Grab all socks and put into red hamper"


python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pile_of_socks_into_red_hamper \
  --policy.type=act \
  --output_dir=outputs/train/pile_of_socks_into_red_hamper1 \
  --job_name=pile_of_socks_into_red_hamper1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav


huggingface-cli upload ${HF_USER}/pile_of_socks_into_red_hamper outputs/train/pile_of_socks_into_red_hamper1/checkpoints/last/pretrained_model --repo-type=model

# evaluate
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pile_of_socks_into_red_hamper \
  --dataset.repo_id=${HF_USER}/eval_pile_of_socks_into_red_hamper \
  --dataset.single_task="Grab all socks and put into red hamper" \
  --dataset.num_episodes=1 \
--display_data=true

########################################################
# July 7th 2025, red cube always in same place, 1 demonstration
########################################################

task_name="red_cube_always_in_same_place"
# run in both laptop and in cloud
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=10 \
    --dataset.single_task="Grab red cube and put to left"

# train in cloud
task_name="red_cube_always_in_same_place"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# upload model to huggingface
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/last/pretrained_model --repo-type=model

# evaluate on laptop
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab red cube and put to left" \
  --dataset.num_episodes=1 \
  --display_data=true


########################################################
# Visualizing datasets
########################################################

# TODO didn't work. 
# `visualize_dataset.py` uses rerun for a detailed, interactive 3D visualization.
# You can inspect camera streams, actions, and states frame-by-frame.
# This command visualizes episode 0 of your sock-hamper dataset.
# Use --video-backend=pyav to avoid torchcodec errors.
python lerobot/scripts/visualize_dataset.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episode-index=0 \
  --video-backend=pyav

# `visualize_dataset_html.py` starts a web server for a simpler HTML-based overview.
# After running, open http://localhost:9090 in your browser.
# This command visualizes your sock-hamper dataset.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper

# You can also select specific episodes to show in the HTML viewer.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episodes 0 2 4




########################################################
# August 16th 2025, 
########################################################

# run in both laptop and in cloud
task_name="pick_place_one_white_sock_black_out_blinds"
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER


# TODO save calibration to repo?
# TODO stop having so many repos.


python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=50 \
    --dataset.single_task="Grab white sock and place into your box"

# resuming
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --resume=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=50 \
    --dataset.single_task="Grab white sock and place into your box"

python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/${task_name}

# train
task_name="pick_place_one_white_sock_black_out_blinds"
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav

# upload model to huggingface, new terminal run hugging cli login again
task_name="pick_place_one_white_sock_black_out_blinds"
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER
run_count=1
#huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/last/pretrained_model --repo-type=model

# 20000
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/020000/pretrained_model --repo-type=model
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/060000/pretrained_model --repo-type=model


# evaluate on laptop white sock resume last
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab white sock and place into your box" \
  --dataset.num_episodes=1 \
  --display_data=true

  updt_s:0.265 with training above on T4
  200 x 0.265 = 53 seconds
  20000 x 0.265 = 5300 seconds = 1.47 hours
  14000 x 0.265 = 3710 seconds = 1.03 hours = 1 hour 4 minutes
  60000 x 0.265 = 15900 seconds = 4.42 hours = 4 hours 25 minutes
  150000 x 0.265 = 39750 seconds = 11.04 hours = 11 hours 3 minutes

Above I mentioned on previous run
ag10 0.092 x 200 = 18.4s per 200 steps
60000 x 0.092 = 5520 seconds = 1.53 hours = 1 hour 32 minutes = almost three times faster

# l40s!!!! 
updt_s:0.051
200 x 0.051 = 10.2 seconds
20000 x 0.051 = 1020 seconds = 0.28 hours = 17 minutes
14000 x 0.051 = 714 seconds = 0.2 hours = 12 minutes
60000 x 0.051 = 3060 seconds = 0.85 hours = 51 minutes
150000 x 0.051 = 7650 seconds = 2.12 hours = 2 hours 7 minutes

# cpu
INFO 2025-08-16 20:30:44 ts/train.py:232 step:200 smpl:2K ep:2 epch:0.04 loss:6.910 grdn:159.099 lr:1.0e-05 updt_s:7.643 data_s:0.014
updt_s:7.643
updt_s:7.643 x 200 = 1528.6 per 200 steps
updt_s:7.643 x 20000 = 152860 seconds = 42.46 hours = 42 hours 28 minutes
updt_s:7.643 x 60000 = 458580 seconds = 127.38 hours = 127 hours 23 minutes


  # TODO try l4go gpu instead of t4

# new train
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav


# TODO learn batch jobs and just run 4 separate jobs parallel and GPU on each one. L40s

program: lerobot/scripts/train.py
method: grid
metric:
  name: train/loss
  goal: minimize
parameters:
  batch_size:
    values: [8, 16, 32, 64, 128]
  steps:
    values: [60000, 30000, 15000, 7500, 3750]
  optimizer_type:
    values: ["adamw"]
  lr:
    values: [1.0e-5, 2.0e-5, 4.0e-5, 8.0e-5, 1.6e-4]
  warmup_steps:
    values: [1000, 800, 600, 400, 300]  # ~5–15% of steps
  decay_steps:
    values: [59000, 29200, 14400, 7000, 3400]
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --dataset.repo_id=${HF_USER}/${task_name}
  - --policy.type=act
  - --policy.device=cuda
  - --policy.use_amp=true
  - --dataset.video_backend=pyav
  - --num_workers=8
  - --batch_size=${batch_size}
  - --steps=${steps}
  - --optimizer.type=${optimizer_type}
  - --optimizer.lr=${lr}
  - --scheduler.type=cosine_decay_with_warmup
  - --scheduler.num_warmup_steps=${warmup_steps}
  - --scheduler.num_decay_steps=${decay_steps}
  - --scheduler.peak_lr=${lr}
  - --scheduler.decay_lr=1e-6
  - --save_checkpoint=true
  - --save_freq=10000
  - --wandb.enable=true


wandb sweep sweep.yaml        # returns SWEEP_ID
wandb agent SWEEP_ID          # run agents (repeat to run more in parallel)

https://docs.wandb.ai/guides/sweeps/define-sweep-configuration/?utm_source=chatgpt.com
https://lightning.ai/docs/overview/scale-with-batch-jobs/CLI?utm_source=chatgpt.com



from lightning docs WOOOOOOW
# Install the Lightning SDK
# pip install -U lightning-sdk

# login to the platform
# export LIGHTNING_USER_ID=171476ec-0416-428a-9ea4-50eb98c0bd4b
# export LIGHTNING_API_KEY=235018dd-badd-47c7-8d1b-e8bbc0270821

from lightning_sdk import Machine, Studio, Job

# Start the studio
s = Studio(name="mega-rl-experiments", teamspace="rl-il-vision-nlp-hacking", user="benfduffy")
print("starting Studio...")
s.start()

# --------------------------
# Example 1: Submit a job
# --------------------------
job = Job.run(name="my_first_job", command="python main.py", machine=Machine.A10G)

# --------------------------
# Example 2: Hyperparameter sweep
# --------------------------
learning_rates = ['0.01', '0.02', '0.03']
for lr in learning_rates:
    job_cmd = f"python main.py --lr {lr}"
    Job.run(command=job_cmd, name="my-sweep-1", machine=Machine.A10G)

# --------------------------
# Example 3: Benchmark model on different GPUs
# --------------------------
machines = [Machine.A10G, Machine.L40S, Machine.A100_X_8]
for machine in machines:
    job_cmd = "python main.py"
    Job.run(command=job_cmd, name="my-benchmark-1", machine=machine)

print("Stopping Studio")
s.stop()


# Todo ONE DAY LEARN DATA CONNECTIONS S3 IN LIGHTNING. to save space. and make jobs start faster?

##### actual batch job for resume needs to be #############

cd lerobot
task_name="pick_place_one_white_sock_black_out_blinds"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name}_resume \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav \
  --resume true

#### 
task_name="pick_place_one_white_sock_black_out_blinds"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.device=cuda \
  --policy.use_amp=true \
  --output_dir=outputs/train/${task_name}_l40s_b64 \
  --job_name=${task_name}_l40s_b64 \
  --wandb.enable=true \
  --dataset.video_backend=pyav \
  --num_workers=8 \
  --batch_size=64 \
  --steps=7500 \
  --optimizer.type=adamw \
  --optimizer.lr=8e-5 \
  --scheduler.type=cosine_decay_with_warmup \
  --scheduler.num_warmup_steps=400 \
  --scheduler.num_decay_steps=7000 \
  --scheduler.peak_lr=8e-5 \
  --scheduler.decay_lr=1e-6 \
  --save_freq=10000



echo "Done"







bash -lc '
set -euo pipefail
export task_name="pick_place_one_white_sock_black_out_blinds"
export run_count=1

# Logins (use secrets you stored in Studio settings)
huggingface-cli login --token "$HUGGINGFACE_TOKEN" --add-to-git-credential || true
wandb login --relogin "$WANDB_API_KEY" || true

# (Optional) If resuming on a fresh machine with no outputs yet, pull last run dir from HF model repo:
# huggingface-cli download ${HF_USER}/${task_name}-runs --repo-type=model \
#   --local-dir "outputs/train/${task_name}_${run_count}" || true

python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name}_resume \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav \
  --resume true \
  --config_path "outputs/train/${task_name}_${run_count}/train_config.json"

echo "Done"
'



# new machine cpu

--policy.repo_id=${HF_USER}/${task_name}_act_resnet18

run_count=1
task_name="pick_place_one_white_sock_black_out_blinds"
python src/lerobot/scripts/train.py \                                             
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.repo_id=${HF_USER}/${task_name}_${run_count}_act_model \
  --output_dir=outputs/train/${task_name}_testesttest_numpy_less_than_2 \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav




run_count=1
task_name="pick_place_one_white_sock_black_out_blinds"
python src/lerobot/scripts/train.py \                                             
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.repo_id=bearlover365/${task_name}_${run_count}_act_model \
  --output_dir=outputs/train/${task_name}_testesttest_numpy_less_than_2 \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav



export HF_USER="bearlover365"                    # your HF handle
export task_name="pick_place_one_white_sock_black_out_blinds"
export run_count=1
export OUT="outputs/train/${task_name}_run${run_count}_numpylt2"
export MODEL_REPO="${HF_USER}/${task_name}_${run_count}_act_model"
python -m lerobot.scripts.train \
  --dataset.repo_id="bearlover365/${task_name}" \
  --policy.type=act \
  --policy.repo_id=${MODEL_REPO} \
  --output_dir="${OUT}" \
  --job_name="${task_name}" \
  --policy.device=cuda \
  --policy.push_to_hub=false \
  --wandb.enable=true \
  --dataset.video_backend=pyav




########################################################
#  SSH INTO BATCH JOB TO UPLOAD CHECKPOINT
########################################################

⚡ ~ task_name="pick_place_one_white_sock_black_out_blinds"
⚡ ~ huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential

usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument
⚡ ~ HF_USER=$(huggingface-cli whoami | head -n 1)
⚡ ~ echo $HF_USER
bearlover365
⚡ ~ run_count=1
⚡ ~ ssource .lightning_studio/.studiorc                                                
⚡ ~ export HUGGINGFACE_TOKEN="...."
⚡ ~ huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
Token is valid (permission: fineGrained).
The token `first` has been saved to /teamspace/studios/this_studio/.cache/huggingface/stored_tokens
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token
Login successful.
The current active token is: `first`





# 