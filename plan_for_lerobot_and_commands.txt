I'm at a LeRobot hackathon, and getting two so101 arms first to:
1. teleoperate (done)
2. teleoperate with camera (done with loose webcam)
3. Record sample dataset (done)
4. Train a policy sample dataset
4.1 will cpu work for small dataset? if not, how to do it in cloud?
5. Test the policy on the robot
6. Work out FPS on CPU or inference over cloud
7. record big dataset of sock or whatever
8. train a policy on the big dataset
9. test it...




Below is a bunch of commands I'm saving to run again for my specific setup

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 1

python lerobot/scripts/configure_motor.py \
  --port /dev/ttyACM0 \
  --brand feetech \
  --model sts3215 \
  --baudrate 1000000 \
  --ID 4

python -m lerobot.setup_motors \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_leader"]'

python lerobot/scripts/control_robot.py \
  --robot.type=so101 \
  --robot.cameras='{}' \
  --control.type=calibrate \
  --control.arms='["main_follower"]'



python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm


python -m lerobot.calibrate \
    --robot.type=so101_follower \
    --robot.port=/dev/tty.usbmodem58760431551 \ 
    --robot.id=my_awesome_follower_arm

python -m lerobot.calibrate \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm


# what worked, 1 camera
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

# 2 cameras
python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true


# record with 1 camera white socks
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_white_sock \
    --dataset.num_episodes=10 \
    --dataset.single_task="Grab the white sock"

# record with 2 camera black and white socks to container
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
    --dataset.num_episodes=60 \
    --dataset.single_task="Grab the black and white sock and put them into the red box" \
    --resume=true


python -m lerobot.replay \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --dataset.repo_id=${HF_USER}/record-test \
    --dataset.episode=0


# training that works in lightning ai
python lerobot/scripts/train.py \
  --dataset.repo_id=${HF_USER}/record-test4_with_permission_changed \
  --policy.type=act \
  --output_dir=outputs/train/act_record-test \
  --job_name=act_record-test3 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav
  
# white sock act
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_white_sock1 \
  --job_name=pick_up_white_sock1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# to resume add e.g.
--resume=true  --config_path=outputs/train/pick_up_blac
k_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model/train_config.json

# black and white socks into red box 2 cameras
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_black_and_white_sock_into_red_box \
  --policy.type=act \
  --output_dir=outputs/train/pick_up_black_and_white_sock_into_red_box \
  --job_name=pick_up_black_and_white_sock_into_red_box1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# white sock smolvla
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pick_up_white_sock \
  --policy.path=lerobot/smolvla_base \
  --batch_size=64 \
  --output_dir=outputs/train/pick_up_white_sock_smolvla_finetune \
  --job_name=pick_up_white_sock_smolvla_finetune \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav



# trying to eval on cpu but with new eval script. never worked.
python -m lerobot.scripts.eval \
        --robot.type=so101_follower \
        --robot.port=/dev/ttyACM0 \
        --robot.id=my_awesome_follower_arm \
        --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
        --policy_path=${HF_USER}/act_record-test \
        --device=cpu

WITH t4 updt_s:0.263 = 0.263 x 200 = 52.6 per 200 steps
With ag10 18.0.092 x 200 = 18.4s per 200 steps
20000 x 0.092 = 1840 seconds = 0.51 hours = 30 minutes
60000 x 0.092 = 5520 seconds = 1.53 hours = 1 hour 32 minutes

smol vla updt_s:0.793 x 200 = 158.6 per 200 steps

# trying to do the same but with record eval way from docs. 
python -m lerobot.record  \
  --robot.type=so100_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.cameras="{ up: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}, side: {type: intelrealsense, serial_number_or_name: 233522074606, width: 640, height: 480, fps: 30}}" \
  --robot.id=my_awesome_follower_arm \
  --display_data=false \
  --dataset.repo_id=$HF_USER/eval_so100 \
  --dataset.single_task="Put lego brick into the transparent box" \
  --policy.path=${HF_USER}/act_record-test

# worked. 1 camera. wait did it?
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --control.policy.path=${HF_USER}/act_record-test \
  --dataset.repo_id=${HF_USER}/eval_act_record_test \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# 2 cameras and socks into red box, 100 episodes
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}, gripper: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 \
  --dataset.repo_id=${HF_USER}/eval_pick_up_black_and_white_sock_into_red_box \
  --dataset.single_task="Grab the black and white sock and put them into the red box" \
  --dataset.num_episodes=1 \
  --display_data=true

# between episodes to reload model
/home/ben/.cache/huggingface/lerobot/bearlover365/eval_pick_up_black_and_white_sock_into_red_box


HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

huggingface-cli upload \         
      ${HF_USER}/act_record-test \
      outputs/train/act_record-test/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload \         
      ${HF_USER}/pick_up_white_sock \
      outputs/train/pick_up_white_sock1/checkpoints/last/pretrained_model \
      --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_white_sock outputs/train/pick_up_white_sock1/checkpoints/last --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_white_sock_smolvla_finetune outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last --repo-type=model

huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box outputs/train/pick_up_black_and_white_sock_into_red_box/checkpoints/last/pretrained_model --repo-type=model
huggingface-cli upload ${HF_USER}/pick_up_black_and_white_sock_into_red_box_100 outputs/train/pick_up_black_and_white_sock_into_red_box_100/checkpoints/last/pretrained_model --repo-type=model




# think it needs pretrained
huggingface-cli upload \
      ${HF_USER}/pick_up_white_sock_smolvla_finetune \
      outputs/train/pick_up_white_sock_smolvla_finetune/checkpoints/last/pretrained_model \
      --repo-type=model


# eval that worked!!
python -m lerobot.record --config_path eval_config.yaml


(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 56,
        "range_min": 730,
        "range_max": 3434
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 793,
        "range_min": 867,
        "range_max": 3210
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -729,
        "range_min": 870,
        "range_max": 3076
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -50,
        "range_min": 945,
        "range_max": 3217
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 38,
        "range_min": 150,
        "range_max": 3750
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -530,
        "range_min": 2047,
        "range_max": 3482
    }




(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/teleoperators/so101_leader/my_awesome_leader_arm.json 
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 1180,
        "range_min": 794,
        "range_max": 3143
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 1368,
        "range_min": 834,
        "range_max": 3168
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -294,
        "range_min": 908,
        "range_max": 3130
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": -676,
        "range_min": 857,
        "range_max": 3136
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 155,
        "range_min": 199,
        "range_max": 4035
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -2013,
        "range_min": 1590,
        "range_max": 2787
    }




latest, trying to fix homing offset problem of shoulder motor?


/home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
(robosuite) ben@ben-zephyrus2 ~/all_projects/lerobot (main)$ cat /home/ben/.cache/huggingface/lerobot/calibration/robots/so101_follower/my_awesome_follower_arm.json
{
    "shoulder_pan": {
        "id": 1,
        "drive_mode": 0,
        "homing_offset": 58,
        "range_min": 732,
        "range_max": 3430
    },
    "shoulder_lift": {
        "id": 2,
        "drive_mode": 0,
        "homing_offset": 813,
        "range_min": 834,
        "range_max": 3183
    },
    "elbow_flex": {
        "id": 3,
        "drive_mode": 0,
        "homing_offset": -769,
        "range_min": 908,
        "range_max": 3121
    },
    "wrist_flex": {
        "id": 4,
        "drive_mode": 0,
        "homing_offset": 35,
        "range_min": 858,
        "range_max": 3128
    },
    "wrist_roll": {
        "id": 5,
        "drive_mode": 0,
        "homing_offset": 33,
        "range_min": 162,
        "range_max": 3772
    },
    "gripper": {
        "id": 6,
        "drive_mode": 0,
        "homing_offset": -523,
        "range_min": 2041,
        "range_max": 3482
    }









########################################################
June 20th 2025, a week after hackathon, a pile of socks in ben's room, controlled lighting, all into red box.
########################################################

python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

# run in both laptop and in cloud
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/pile_of_socks_into_red_hamper \
    --dataset.num_episodes=5 \
    --dataset.single_task="Grab all socks and put into red hamper"


python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/pile_of_socks_into_red_hamper \
  --policy.type=act \
  --output_dir=outputs/train/pile_of_socks_into_red_hamper1 \
  --job_name=pile_of_socks_into_red_hamper1 \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav


huggingface-cli upload ${HF_USER}/pile_of_socks_into_red_hamper outputs/train/pile_of_socks_into_red_hamper1/checkpoints/last/pretrained_model --repo-type=model

# evaluate
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/pile_of_socks_into_red_hamper \
  --dataset.repo_id=${HF_USER}/eval_pile_of_socks_into_red_hamper \
  --dataset.single_task="Grab all socks and put into red hamper" \
  --dataset.num_episodes=1 \
--display_data=true

########################################################
# July 7th 2025, red cube always in same place, 1 demonstration
########################################################

task_name="red_cube_always_in_same_place"
# run in both laptop and in cloud
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=10 \
    --dataset.single_task="Grab red cube and put to left"

# train in cloud
task_name="red_cube_always_in_same_place"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav

# upload model to huggingface
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/last/pretrained_model --repo-type=model

# evaluate on laptop
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ front: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab red cube and put to left" \
  --dataset.num_episodes=1 \
  --display_data=true


########################################################
# Visualizing datasets
########################################################

# TODO didn't work. 
# `visualize_dataset.py` uses rerun for a detailed, interactive 3D visualization.
# You can inspect camera streams, actions, and states frame-by-frame.
# This command visualizes episode 0 of your sock-hamper dataset.
# Use --video-backend=pyav to avoid torchcodec errors.
python lerobot/scripts/visualize_dataset.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episode-index=0 \
  --video-backend=pyav

# `visualize_dataset_html.py` starts a web server for a simpler HTML-based overview.
# After running, open http://localhost:9090 in your browser.
# This command visualizes your sock-hamper dataset.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper

# You can also select specific episodes to show in the HTML viewer.
python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/pile_of_socks_into_red_hamper \
  --episodes 0 2 4




########################################################
# August 16th 2025, beginnings of mobile manipulation, but starting static
########################################################

# run in both laptop and in cloud
task_name="pick_place_one_white_sock_black_out_blinds"
task_name="pick_place_up_to_four_white_socks_black_out_blinds"
task_name="pick_place_up_to_four_white_socks_varying_daylight_intensity"


huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER


# TODO save calibration to repo?
# TODO stop having so many repos.


python -m lerobot.teleoperate \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true

python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=51 \
    --dataset.single_task="Grab all socks and place into your box"

# resuming
python -m lerobot.record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --resume=true \
    --dataset.repo_id=${HF_USER}/${task_name} \
    --dataset.num_episodes=70 \
    --dataset.single_task="Grab white sock and place into your box"

python lerobot/scripts/visualize_dataset_html.py \
  --repo-id=${HF_USER}/${task_name}

# train
task_name="pick_place_one_white_sock_black_out_blinds"
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER

run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav

# upload model to huggingface, new terminal run hugging cli login again
task_name="pick_place_one_white_sock_black_out_blinds"
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
HF_USER=$(huggingface-cli whoami | head -n 1)
echo $HF_USER
run_count=1
#huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/last/pretrained_model --repo-type=model

# 20000
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/020000/pretrained_model --repo-type=model
huggingface-cli upload ${HF_USER}/${task_name} outputs/train/${task_name}_${run_count}/checkpoints/060000/pretrained_model --repo-type=model


# phospho lerobot dataset docs
https://docs.phospho.ai/learn/lerobot-dataset

# evaluate on laptop white sock resume last
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab white sock and place into your box" \
  --dataset.num_episodes=1 \
  --resume true \
  --display_data=true

# evaluate specific step number mode name on laptop white sock resume last
saved_model_step_number=130000
task_name="pick_place_one_white_sock_black_out_blinds"
HF_USER=bearlover365
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name}_${saved_model_step_number} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab white sock and place into your box" \
  --dataset.num_episodes=1 \
  --resume true \
  --display_data=true


# evaluating d1 + d2 combined 75k
saved_model_step_number=075000
#task_name="d1_and_d2_datasets_act_224x224"
#task_name="bearlover365/d1_d2_act224_s500k_b8_ckpt25k_075000"
task_name="d1_d2_act224_s500k_b8_ckpt25k"

#task_name="pick_place_up_to_four_white_socks_black_out_blinds"
HF_USER=bearlover365
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name}_${saved_model_step_number} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab all socks and place into your box" \
  --dataset.num_episodes=1 \
  --resume true \
  --display_data=true

python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${HF_USER}/${task_name}_${saved_model_step_number} \
  --dataset.repo_id=${HF_USER}/eval_${task_name} \
  --dataset.single_task="Grab all socks and place into your box" \
  --dataset.num_episodes=1 \
  --resume true \
  --dataset.revision=0.2.0
  --display_data=true

# eval that actually worked without all dataset problems
# choose a local root so nothing touches the Hub
export HF_USER=bearlover365
export task_name="d1_d2_act224_s500k_b8_ckpt25k"
export EVAL_ROOT="$HOME/.cache/lerobot_local/eval_${task_name}"
# Optional: ensure a clean slate locally
rm -rf "$EVAL_ROOT"
model_step_number=250000
# Run record WITHOUT --resume and with push_to_hub=false
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=bearlover365/${task_name}_${model_step_number} \
  --dataset.repo_id=bearlover365/eval_${task_name} \
  --dataset.root="$EVAL_ROOT" \
  --dataset.single_task="Grab all socks and place into your box" \
  --dataset.num_episodes=1 \
  --dataset.push_to_hub=false \
  --resume=false \
  --display_data=true

export EVAL_ROOT="$HOME/.cache/lerobot_local/eval_${task_name}"
# trying to avoid a new variable and make it easy

py310
task_name="d2_original_good_act_1_act_model"
task_name="d1_d2_merged_normal_resolution_300000"


task_name="d4_dataset_only_2_validation_episodes"
full_model_path="bearlover365/${task_name}_200000"
full_model_path="bearlover365/${task_name}"
python -m lerobot.record \
  --robot.type=so101_follower \
  --robot.port=/dev/ttyACM0 \
  --robot.id=my_awesome_follower_arm \
  --robot.cameras="{ wrist: {type: opencv, index_or_path: 4, width: 640, height: 480, fps: 30}}" \
  --policy.path=${full_model_path} \
  --dataset.repo_id=bearlover365/eval_${task_name} \
  --dataset.root="$HOME/.cache/lerobot_local/eval_${task_name}" \
  --dataset.single_task="Grab all socks and place into your box" \
  --dataset.num_episodes=1 \
  --dataset.push_to_hub=false \
  --resume=false \
  --display_data=true

# TODO I think it says it while recording? make sure cpu has enough speed to implement GPU model! profile!
# TODO never tried replay just to test repeatability.

updt_s:0.265 with training above on T4
200 x 0.265 = 53 seconds
20000 x 0.265 = 5300 seconds = 1.47 hours
14000 x 0.265 = 3710 seconds = 1.03 hours = 1 hour 4 minutes
60000 x 0.265 = 15900 seconds = 4.42 hours = 4 hours 25 minutes
150000 x 0.265 = 39750 seconds = 11.04 hours = 11 hours 3 minutes

Above I mentioned on previous run
ag10 0.092 x 200 = 18.4s per 200 steps
60000 x 0.092 = 5520 seconds = 1.53 hours = 1 hour 32 minutes = almost three times faster

# l40s!!!! 
updt_s:0.051
200 x 0.051 = 10.2 seconds
20000 x 0.051 = 1020 seconds = 0.28 hours = 17 minutes
14000 x 0.051 = 714 seconds = 0.2 hours = 12 minutes
60000 x 0.051 = 3060 seconds = 0.85 hours = 51 minutes
150000 x 0.051 = 7650 seconds = 2.12 hours = 2 hours 7 minutes

# imaginary l40s 3x faster with 224 resolution
0.051 / 3 = 0.017
200 x 0.017 = 3.4 seconds
20000 x 0.017 = 340 seconds = 0.09 hours = 5 minutes
14000 x 0.017 = 238 seconds = 0.06 hours = 4 minutes
60000 x 0.017 = 1020 seconds = 0.28 hours = 17 minutes
150000 x 0.017 = 2550 seconds = 0.71 hours = 43 minutes


# cpu
INFO 2025-08-16 20:30:44 ts/train.py:232 step:200 smpl:2K ep:2 epch:0.04 loss:6.910 grdn:159.099 lr:1.0e-05 updt_s:7.643 data_s:0.014
updt_s:7.643
updt_s:7.643 x 200 = 1528.6 per 200 steps
updt_s:7.643 x 20000 = 152860 seconds = 42.46 hours = 42 hours 28 minutes
updt_s:7.643 x 60000 = 458580 seconds = 127.38 hours = 127 hours 23 minutes


# later cpu but image resolution act 224x224 bash script.
INFO 2025-08-17 12:16:25 ts/train.py:232 step:10 smpl:80 ep:0 epch:0.00 loss:41.915 grdn:639.243 lr:1.0e-05 updt_s:63.137 data_s:0.092
ahh something was wrong there, think it was 2.2s/step... 


# new train
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav


# TODO learn batch jobs and just run 4 separate jobs parallel and GPU on each one. L40s

program: lerobot/scripts/train.py
method: grid
metric:
  name: train/loss
  goal: minimize
parameters:
  batch_size:
    values: [8, 16, 32, 64, 128]
  steps:
    values: [60000, 30000, 15000, 7500, 3750]
  optimizer_type:
    values: ["adamw"]
  lr:
    values: [1.0e-5, 2.0e-5, 4.0e-5, 8.0e-5, 1.6e-4]
  warmup_steps:
    values: [1000, 800, 600, 400, 300]  # ~5–15% of steps
  decay_steps:
    values: [59000, 29200, 14400, 7000, 3400]
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - --dataset.repo_id=${HF_USER}/${task_name}
  - --policy.type=act
  - --policy.device=cuda
  - --policy.use_amp=true
  - --dataset.video_backend=pyav
  - --num_workers=8
  - --batch_size=${batch_size}
  - --steps=${steps}
  - --optimizer.type=${optimizer_type}
  - --optimizer.lr=${lr}
  - --scheduler.type=cosine_decay_with_warmup
  - --scheduler.num_warmup_steps=${warmup_steps}
  - --scheduler.num_decay_steps=${decay_steps}
  - --scheduler.peak_lr=${lr}
  - --scheduler.decay_lr=1e-6
  - --save_checkpoint=true
  - --save_freq=10000
  - --wandb.enable=true


wandb sweep sweep.yaml        # returns SWEEP_ID
wandb agent SWEEP_ID          # run agents (repeat to run more in parallel)

https://docs.wandb.ai/guides/sweeps/define-sweep-configuration/?utm_source=chatgpt.com
https://lightning.ai/docs/overview/scale-with-batch-jobs/CLI?utm_source=chatgpt.com



# lightning upload files between studios
swapped from gcp to aws so i could run L40s and do 150k steps in 2 hours
lightning upload folder outputs/train/d2_original_good_act_1 --remote-path lerobot/outputs/train/d2_original_good_act_1

from lightning docs WOOOOOOW
# Install the Lightning SDK
# pip install -U lightning-sdk

# login to the platform
# export LIGHTNING_USER_ID=171476ec-0416-428a-9ea4-50eb98c0bd4b
# export LIGHTNING_API_KEY=235018dd-badd-47c7-8d1b-e8bbc0270821

from lightning_sdk import Machine, Studio, Job

# Start the studio
s = Studio(name="mega-rl-experiments", teamspace="rl-il-vision-nlp-hacking", user="benfduffy")
print("starting Studio...")
s.start()

# --------------------------
# Example 1: Submit a job
# --------------------------
job = Job.run(name="my_first_job", command="python main.py", machine=Machine.A10G)

# --------------------------
# Example 2: Hyperparameter sweep
# --------------------------
learning_rates = ['0.01', '0.02', '0.03']
for lr in learning_rates:
    job_cmd = f"python main.py --lr {lr}"
    Job.run(command=job_cmd, name="my-sweep-1", machine=Machine.A10G)

# --------------------------
# Example 3: Benchmark model on different GPUs
# --------------------------
machines = [Machine.A10G, Machine.L40S, Machine.A100_X_8]
for machine in machines:
    job_cmd = "python main.py"
    Job.run(command=job_cmd, name="my-benchmark-1", machine=machine)

print("Stopping Studio")
s.stop()


# Todo ONE DAY LEARN DATA CONNECTIONS S3 IN LIGHTNING. to save space. and make jobs start faster?

##### actual batch job for resume needs to be #############

cd lerobot
task_name="pick_place_one_white_sock_black_out_blinds"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name}_resume \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav \
  --resume true

#### 
task_name="pick_place_one_white_sock_black_out_blinds"
run_count=1
python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.device=cuda \
  --policy.use_amp=true \
  --output_dir=outputs/train/${task_name}_l40s_b64 \
  --job_name=${task_name}_l40s_b64 \
  --wandb.enable=true \
  --dataset.video_backend=pyav \
  --num_workers=8 \
  --batch_size=64 \
  --steps=7500 \
  --optimizer.type=adamw \
  --optimizer.lr=8e-5 \
  --scheduler.type=cosine_decay_with_warmup \
  --scheduler.num_warmup_steps=400 \
  --scheduler.num_decay_steps=7000 \
  --scheduler.peak_lr=8e-5 \
  --scheduler.decay_lr=1e-6 \
  --save_freq=10000



echo "Done"







bash -lc '
set -euo pipefail
export task_name="pick_place_one_white_sock_black_out_blinds"
export run_count=1

# Logins (use secrets you stored in Studio settings)
huggingface-cli login --token "$HUGGINGFACE_TOKEN" --add-to-git-credential || true
wandb login --relogin "$WANDB_API_KEY" || true

# (Optional) If resuming on a fresh machine with no outputs yet, pull last run dir from HF model repo:
# huggingface-cli download ${HF_USER}/${task_name}-runs --repo-type=model \
#   --local-dir "outputs/train/${task_name}_${run_count}" || true

python lerobot/scripts/train.py \
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --output_dir=outputs/train/${task_name}_${run_count} \
  --job_name=${task_name}_resume \
  --policy.device=cuda \
  --wandb.enable=true \
  --save_freq=10000 \
  --dataset.video_backend=pyav \
  --resume true \
  --config_path "outputs/train/${task_name}_${run_count}/train_config.json"

echo "Done"
'



# new machine cpu

--policy.repo_id=${HF_USER}/${task_name}_act_resnet18

run_count=1
task_name="pick_place_one_white_sock_black_out_blinds"
python src/lerobot/scripts/train.py \                                             
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.repo_id=${HF_USER}/${task_name}_${run_count}_act_model \
  --output_dir=outputs/train/${task_name}_testesttest_numpy_less_than_2 \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav




run_count=1
task_name="pick_place_one_white_sock_black_out_blinds"
python src/lerobot/scripts/train.py \                                             
  --dataset.repo_id=bearlover365/${task_name} \
  --policy.type=act \
  --policy.repo_id=bearlover365/${task_name}_${run_count}_act_model \
  --output_dir=outputs/train/${task_name}_testesttest_numpy_less_than_2 \
  --job_name=${task_name} \
  --policy.device=cuda \
  --wandb.enable=true \
  --dataset.video_backend=pyav



export HF_USER="bearlover365"                    # your HF handle
export task_name="pick_place_one_white_sock_black_out_blinds"
export run_count=1
export OUT="outputs/train/${task_name}_run${run_count}_numpylt2"
export MODEL_REPO="${HF_USER}/${task_name}_${run_count}_act_model"
python -m lerobot.scripts.train \
  --dataset.repo_id="bearlover365/${task_name}" \
  --policy.type=act \
  --policy.repo_id=${MODEL_REPO} \
  --output_dir="${OUT}" \
  --job_name="${task_name}" \
  --policy.device=cuda \
  --policy.push_to_hub=false \
  --wandb.enable=true \
  --dataset.video_backend=pyav




########################################################
#  SSH INTO BATCH JOB TO UPLOAD CHECKPOINT
########################################################

⚡ ~ task_name="pick_place_one_white_sock_black_out_blinds"
⚡ ~ huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential



usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]
huggingface-cli <command> [<args>] login: error: argument --token: expected one argument
⚡ ~ HF_USER=$(huggingface-cli whoami | head -n 1)
⚡ ~ echo $HF_USER
bearlover365
⚡ ~ run_count=1
⚡ ~ ssource .lightning_studio/.studiorc                                                
⚡ ~ export HUGGINGFACE_TOKEN="...."
⚡ ~ huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
Token is valid (permission: fineGrained).
The token `first` has been saved to /teamspace/studios/this_studio/.cache/huggingface/stored_tokens
Your token has been saved in your configured git credential helpers (store).
Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token
Login successful.
The current active token is: `first`





# ###### batch jobs commands and shit 
lightning list jobs --teamspace benfduffy/rl-il-vision-nlp-hacking

lightning inspect job --name act-224x224-s1k --teamspace benfduffy/rl-il-vision-nlp-hacking

lightning run job \
  --teamspace benfduffy/rl-il-vision-nlp-hacking \
  --studio only_lerobot \
  --machine L40S \
  --name act-224x224- \
  --command "STEPS=1000 LOG_FREQ=10 BATCH=64 bash /teamspace/studios/this_studio/lerobot/jobs/run_act_224.sh"

bash /teamspace/studios/this_studio/lerobot/jobs/run_act_224_run_on_d1_d2.sh

###############
# all install commands on new machine 
###############################

copy studiorc  # seems auto copied cool. 

git clone https://github.com/beduffy/lerobot
huggingface-cli login --token "$HUGGINGFACE_TOKEN" --add-to-git-credential || true
hf auth whoami
conda install ffmpeg -c conda-forge
wandb login --relogin "$WANDB_API_KEY" || true
cd lerobot
pip install -e .


#############
###### dataset plan
########################

TODO
- have more non sock objects around that it shouldn't lift e.g. my slippers or? 
- Is create3 in office? bring it home. Will it be easy to drive and attach scissor lift to? will it be too unstable? Should bring it anyway even for D7 and D8... 

gpt5 ablation:
- Eval you can trust (so ablations mean something)
- Make a tiny, repeatable eval suite (20 trials per cell is great):
- Color generalization: train white; test gray/black.
- Lighting: train blackout; test daylight (3 intensities).
- Camera height: test the D3 heights not in train.
- Pose shifts: test 4–6 robot/table poses unseen in train.
- Backgrounds: test in the new room(s).
- Moving base: if applicable, test mild base motion.

Metrics: success rate, time‑to‑first grasp, number of re‑grips, and “abort” count. Log per‑cell so you can see exactly which factor hurts.
Ablations to actually run:

D1 only vs D1+D2 vs D1–D4 (white→lighting).
+D5/D6 (add color).
+D7 (pose randomization).
Hold out entire factors in training to answer your questions (“never saw gray”, “never saw daytime”, “never moved platform”).



Datasets (10-200 demos each?): 
DONE.
D1. 51 demos (0-50) of Fixed robot facing white wall, blackout blinds, overhead light, only 1 white sock in many positions. Mostly looking at rerun rather than the robot to make it harder and more interesting. 
bearlover365/pick_place_one_white_sock_black_out_blinds
D2. 51 demos of Fixed robot facing white wall, blackout blinds, overhead light, 1-4 slightly different white socks in many positions. More details: recording, actually did lots of fails and retries and couldn't reach to the right. I worry too many retries. And tried looking around and picking up multiple socks too. took 75 minutes probably. 51 demos. num_frames=83031 
bearlover365/pick_place_up_to_four_white_socks_black_out_blinds
D4. (daylight, no overheads, white socks): Vary daylight intensity (blinds high/half/low) across episodes. 40–60 eps. Fix issue with socks on right! TODO.  Aim ~150–200 successful picks in total. same as above but no flashy lights or overhead light, this time daylight with blackout blinds up. Also try different amounts of daylight for different episodes. actually did much more variation, even funky lighting and moving lamp everywhere, door open closed, many different blind configurations, very dark, oh my, even blinds down, crazy lamp etc
bearlover365/pick_place_up_to_four_white_socks_varying_daylight_intensity
two validation episodes were 0 and 72, one daylight and 2nd pretty dark. 

To be recorded:
D3. (scissor heights): 3–4 distinct wrist heights × ~12 eps each ⇒ 36–48 eps. Make sure each height has ~20 successful picks.
D5. blackout blinds down, but many different socks, big ones small ones, many different colours and up to 10 of them. 
D6. Same as above but daylight. 
D7. Then start moving the platform to many different places in my room, with sometimes daylight and sometimes blackout blinds (or two different datasets for this?). MANY DIFFERENT POSITIONS, needs way more data, maybe 500 episodes?  TODO. 
D8. Then start moving to entire apartment. Hopefully with enough training of the above, the model has learned to not really care about the background/environment and more learned what socks look like and what it looks like to have a succesful grasp. No? How much will background screw me? I want to find a way to quantify this and find tricks to need less data. Maybe I can do 1 room at a time and see how 1 extra room helps.










###### overall plan from dataset perspective

Short answer: you can do “train D1 → fine-tune on D2 → … → fine-tune on D8,” but don’t do it without replay. Pure sequential fine-tuning forgets earlier invariances and overfits to the last dataset. The most reliable recipe for max generalization is:

Curriculum + replay (a.k.a. annealed co-training), then

A short union fine-tune on all data, and

Auto-retry at test time (confidence-gated), optionally followed by

A small HIL-SERL online polish with a reward classifier.

Below is a concrete, lean plan to push toward your “~99% with ≤2 retries” goal with ~1k demos instead of 2k.

A. Curriculum vs co-training — what to do

Do: train on D1 (get it working), fine-tune on D2 while replaying 20–30% of D1 in the batches, then move to D7/D8 with a 10–20% replay from everything before.

Finish with a short union pass: 3–8 “epochs” over the entire mixture (light LR) so the policy consolidates across scenes.

Don’t: sequentially fine-tune with zero replay; you’ll lose earlier robustness.

Why this beats equal co-training from scratch: you get the stability of a curriculum (easier → harder) and you prevent forgetting with replay. It also needs fewer total steps than heavy simultaneous mixing.

B. Data plan to hit ~99% with retries (≈1k demos)

Target ~1,000 demos total, but make every demo count:

D1 (clean single-sock): 15–20 (just for pipeline).

D2 (1–4 white socks, same setup): 60–100.

D5/D6 (multi-sock, varied colors; blinds down/up): 120–200 total.

D7 (many placements in your room, day/night): 400–500.

D8 (apartment-wide): the rest to 1k.

Make 1k act like 2k (dataset curation)

Cover placements first: ensure every corner/wall/floor/lighting mode shows up; this buys more than extra repeats in one spot.

Core-set selection: embed frames with your vision backbone (or DINO), pick diverse demos via k-center / k-medoids over (placement, lighting, floor, sock-ID) bins—drop near-duplicates.

Hard-negative mining: keep failures and near-misses; they teach recovery and “don’t grab air.”

Balance long tail: overweight dark socks / near-wall cases / shiny floors.

Augment lightly: 15–25% photometric jitter, small blur/noise; avoid heavy geometric warps on the gripper/sock. Real scene diversity > synthetic background swaps.

C. Model + training knobs (ACT)

Input: your wrist cam downsampled to 160×160 (or 192).

Chunk length H: 50–60 at 10–20 Hz.

Batch: 8–12.

Schedule: cosine to ~3e-6 by the end of each stage; enable EMA.

Sampler: if available, sample by attempts (grasp events), not raw frames; otherwise cap per-episode samples so long D2 episodes don’t dominate.

Confidence head (simple): add a tiny MLP off the policy features to predict grasp-success probability at t+K. Use it only for runtime gating (see next).

E. Optional HIL-SERL polish (fewer demos, better tails)

Train a reward classifier from your recorded data (“sock in gripper/on deck”).

Run 1–2 short online sessions: the robot tries; you intervene occasionally; it learns to retry because reward only comes when a sock is truly stowed.

Use this after the curriculum to lift stubborn edge cases; you don’t have to re-collect lots of new demos.

F. “Train one set to good, then next?” — exact schedule

Here’s a concrete 3-stage schedule that’s fast and safe:

Stage 1 (D1→D2):

Train on D1+ D2 with D2: 80%, D1: 20% sampling, ~12–16 epochs over the mix.

Eval on 3 held-out test placements.

Stage 2 (add D5/D6 + D7):

Fine-tune with D7: 60%, D5/D6: 25%, D2: 10%, D1: 5% for ~8–12 epochs.

Add the confidence head; calibrate τ on held-out trials.

Stage 3 (add D8 apartment-wide):

Fine-tune with D8: 70%, replay (D7: 20%, D5/D6: 10%) for ~6–8 epochs.

Union consolidation: last 3–5 epochs over all data at 1/3 LR.

If you prefer pure sequential fine-tune, keep the replay tail shown above (that’s the key difference vs risky “train only on the latest”).

G. When to stop / how to tell if 12 epochs are enough

Use the same formula you used before: steps ≈ (epochs × num_frames) / batch.

Start with ~12 epochs after each merge; if real-world success is still rising at eval checkpoints, extend to 16–20. If it stalls, don’t waste steps—move to the next stage or gather targeted data.

H. RL vs IL to reach 99%

IL + two-retry wrapper will often get you ~99% perceived on in-distribution rooms with ~1k good demos.

HIL-SERL helps when you want that last few % on weird corners/dark socks with fewer extra demos; use it as a finisher, not a replacement for coverage.

I. Quick “1k instead of 2k” checklist

Prioritize placement diversity over repeats.

Use core-set selection to remove near-dupes.

Keep hard negatives and edge cases.

Add a confidence head + two-retry wrapper.

Optionally do one short HIL-SERL session at the end.

If you give me the current dataset.num_frames after you add D2, I’ll spit out exact step counts for 12/16/20 epochs and a ready-to-paste sampling weight table for your next two stages.